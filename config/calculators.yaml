# config/calculators.yaml
# Configuration for "Help Me Estimate" calculators

# Which calculators are enabled and in what order
enabled_calculators:
  storage:
    - microscopy
    - photography
    - genomics
    - video
    - medical-imaging
    - documents

  cpu:
    - genomics-pipelines
    - simulations
    - batch-processing
    - statistics

  gpu:
    - ml-training
    - ml-inference
    - gpu-simulation

  api:
    - llm-api-costs

# Calculator-specific configuration
calculator_config:
  # ============================================================
  # STORAGE CALCULATORS
  # ============================================================

  microscopy:
    name: "Microscopy Images"
    icon: "microscope"
    description: "Confocal, fluorescence, electron microscopy"

    # Institution-specific defaults
    default_resolution: "4k"
    default_bit_depth: 16

    # Quick presets for common setups
    presets:
      - label: "Confocal Core"
        resolution: "4k"
        bit_depth: 16
        channels: 4
        description: "Standard confocal microscope settings"
      - label: "Light Sheet"
        resolution: "4k"
        bit_depth: 16
        channels: 2
        z_slices: 200
        description: "Light sheet microscopy with Z-stack"
      - label: "Electron Microscopy"
        resolution: "8k"
        bit_depth: 16
        channels: 1
        description: "High-resolution EM images"

    # Resolution options
    resolutions:
      - label: "2K (2048 x 2048)"
        key: "2k"
        pixels: 4194304
      - label: "4K (4096 x 4096)"
        key: "4k"
        pixels: 16777216
      - label: "8K (8192 x 8192)"
        key: "8k"
        pixels: 67108864

    # Bit depth options
    bit_depths:
      - label: "8-bit (grayscale)"
        value: 8
        bytes_per_pixel: 1
      - label: "16-bit (standard)"
        value: 16
        bytes_per_pixel: 2
      - label: "32-bit (float)"
        value: 32
        bytes_per_pixel: 4

  photography:
    name: "Photography/Fieldwork"
    icon: "camera"
    description: "Field photos, specimen imaging, documentation"

    presets:
      - label: "DSLR RAW"
        size_mb: 50
        description: "Full-resolution RAW files"
      - label: "DSLR JPEG"
        size_mb: 12
        description: "High-quality JPEG"
      - label: "Smartphone"
        size_mb: 5
        description: "Modern smartphone photos"
      - label: "Drone Aerial"
        size_mb: 25
        description: "Drone photography"

  genomics:
    name: "Genomics Data"
    icon: "dna"
    description: "Sequencing reads, alignments, variants"

    # Which data types to show
    data_types:
      - label: "Whole Genome (30x)"
        size_gb: 150
        description: "Raw FASTQ + BAM + VCF"
      - label: "Whole Exome"
        size_gb: 20
        description: "Raw + processed files"
      - label: "RNA-seq"
        size_gb: 30
        description: "Per sample, raw + counts"
      - label: "Single-cell RNA"
        size_gb: 100
        description: "10x Chromium, per run"
      - label: "ATAC-seq"
        size_gb: 25
        description: "Per sample"
      - label: "ChIP-seq"
        size_gb: 15
        description: "Per sample"
      - label: "Metagenomics"
        size_gb: 50
        description: "Per sample, variable"

  video:
    name: "Video Recording"
    icon: "video"
    description: "Behavioral studies, time-lapse, lectures"

    presets:
      - label: "HD 1080p"
        gb_per_hour: 10
        description: "Standard HD video"
      - label: "4K"
        gb_per_hour: 45
        description: "Ultra HD video"
      - label: "Time-lapse (compressed)"
        gb_per_hour: 2
        description: "Compressed time-lapse"
      - label: "High-speed (1000fps)"
        gb_per_hour: 200
        description: "Scientific high-speed camera"

  medical-imaging:
    name: "Medical Imaging"
    icon: "scan"
    description: "CT, MRI, PET, X-ray, Ultrasound"

    data_types:
      - label: "CT Scan"
        size_gb: 0.5
        description: "Per study, DICOM"
      - label: "MRI (structural)"
        size_gb: 1
        description: "Per study"
      - label: "fMRI"
        size_gb: 5
        description: "Per session"
      - label: "PET Scan"
        size_gb: 0.3
        description: "Per study"
      - label: "Whole Slide Imaging"
        size_gb: 3
        description: "Per slide, 40x magnification"

  documents:
    name: "Documents & PDFs"
    icon: "file-text"
    description: "Papers, reports, text documents"

    presets:
      - label: "Text document"
        size_mb: 0.1
        description: "Plain text or simple doc"
      - label: "PDF (text-only)"
        size_mb: 0.5
        description: "Standard PDF"
      - label: "PDF with images"
        size_mb: 5
        description: "PDF with embedded figures"
      - label: "Scanned document"
        size_mb: 20
        description: "High-resolution scan"

  # ============================================================
  # CPU CALCULATORS
  # ============================================================

  genomics-pipelines:
    name: "Genomics Pipelines"
    icon: "workflow"
    description: "Alignment, variant calling, RNA-seq"

    pipelines:
      - label: "WGS Alignment (BWA-MEM2)"
        su_per_sample: 300
        description: "30x genome alignment"
      - label: "Variant Calling (GATK)"
        su_per_sample: 200
        description: "Per sample, joint calling extra"
      - label: "RNA-seq (STAR + featureCounts)"
        su_per_sample: 50
        description: "Alignment + quantification"
      - label: "Single-cell (Cell Ranger)"
        su_per_sample: 400
        description: "10x processing pipeline"
      - label: "Differential Expression (DESeq2)"
        su_per_sample: 10
        description: "Statistical analysis"
      - label: "Metagenomics (MetaPhlAn)"
        su_per_sample: 100
        description: "Community profiling"

  simulations:
    name: "Scientific Simulations"
    icon: "atom"
    description: "GROMACS, LAMMPS, OpenFOAM, ANSYS"

    packages:
      - label: "GROMACS (MD)"
        su_per_ns_per_million_atoms: 100
        typical_runs: "1-100 ns"
        description: "Molecular dynamics"
      - label: "LAMMPS"
        su_per_ns_per_million_atoms: 80
        description: "General molecular dynamics"
      - label: "OpenFOAM (CFD)"
        su_per_hour_simulated: 500
        description: "Computational fluid dynamics"
      - label: "ANSYS Fluent"
        su_per_hour_simulated: 1000
        description: "Commercial CFD"
      - label: "COMSOL"
        su_per_hour_simulated: 800
        description: "Multi-physics"
      - label: "Gaussian"
        su_per_calculation: 50
        description: "Quantum chemistry, varies by method"

  batch-processing:
    name: "General Batch Processing"
    icon: "layers"
    description: "Custom scripts, image processing, data conversion"

    templates:
      - label: "Light processing"
        su_per_file: 0.1
        description: "File conversion, simple transforms"
      - label: "Medium processing"
        su_per_file: 1
        description: "Image analysis, data extraction"
      - label: "Heavy processing"
        su_per_file: 10
        description: "Complex analysis per file"

  statistics:
    name: "Statistical Analysis"
    icon: "bar-chart"
    description: "R, Stata, SAS, SPSS"

    workloads:
      - label: "Basic statistics"
        su_estimate: 10
        description: "T-tests, ANOVA, regression"
      - label: "Mixed models"
        su_estimate: 100
        description: "lme4, multilevel models"
      - label: "Bayesian MCMC"
        su_estimate: 1000
        description: "Stan, JAGS, MCMC sampling"
      - label: "Bootstrapping (10K)"
        su_estimate: 500
        description: "Resampling methods"

  # ============================================================
  # GPU CALCULATORS
  # ============================================================

  ml-training:
    name: "ML Training"
    icon: "brain"
    description: "Deep learning model training"

    model_sizes:
      - label: "Small (ResNet-18, BERT-base)"
        typical_hours: 10
        description: "Fine-tuning or small datasets"
      - label: "Medium (ResNet-50, GPT-2)"
        typical_hours: 50
        description: "Full training, medium datasets"
      - label: "Large (ViT-L, LLaMA-7B)"
        typical_hours: 200
        description: "Large models, big datasets"
      - label: "Very Large (LLaMA-70B)"
        typical_hours: 2000
        description: "Requires multi-GPU"

    # Factors that affect GPU time
    factors:
      - "Dataset size"
      - "Number of epochs"
      - "Model complexity"
      - "Hyperparameter tuning runs"

  ml-inference:
    name: "ML Inference"
    icon: "zap"
    description: "Running trained models on data"

    workloads:
      - label: "Image classification"
        items_per_gpu_hour: 50000
        description: "ResNet-50 inference"
      - label: "Object detection"
        items_per_gpu_hour: 5000
        description: "YOLO, Faster R-CNN"
      - label: "Text generation"
        tokens_per_gpu_hour: 100000
        description: "LLM inference (7B model)"
      - label: "Embedding generation"
        items_per_gpu_hour: 20000
        description: "BERT embeddings"

  gpu-simulation:
    name: "GPU-Accelerated Simulation"
    icon: "cpu"
    description: "CUDA-accelerated scientific codes"

    packages:
      - label: "GROMACS (GPU)"
        speedup: "5-10x vs CPU"
        gpu_hours_per_ns: 1
        description: "GPU-accelerated MD"
      - label: "AMBER (GPU)"
        speedup: "5-20x vs CPU"
        gpu_hours_per_ns: 0.5
        description: "Biomolecular MD"
      - label: "NAMD"
        speedup: "3-5x vs CPU"
        gpu_hours_per_ns: 2
        description: "Large-scale MD"

  # ============================================================
  # API COST CALCULATORS
  # ============================================================

  llm-api-costs:
    name: "LLM API Costs"
    icon: "message-square-text"
    description: "Estimate costs for commercial LLM APIs (OpenAI, Anthropic, Google, DeepSeek). OpenAI models also available via Microsoft AI Foundry."

    # Pricing as of February 2026 - update periodically
    # Prices are per million tokens (direct API pricing)
    # Azure AI Foundry uses same per-token rates as OpenAI direct
    # No academic discount on per-token pricing; check institutional agreements
    models:
      # OpenAI
      - label: "GPT-5.3"
        provider: "OpenAI"
        input_per_million: 1.25
        output_per_million: 10.00
        context_window: 400000
        description: "Latest flagship model, strong general performance"
      - label: "GPT-5 Mini"
        provider: "OpenAI"
        input_per_million: 0.25
        output_per_million: 2.00
        context_window: 200000
        description: "Balanced cost and capability"
      - label: "GPT-4o"
        provider: "OpenAI"
        input_per_million: 2.50
        output_per_million: 10.00
        context_window: 128000
        description: "Previous gen multimodal, still widely used"
      - label: "GPT-4o mini"
        provider: "OpenAI"
        input_per_million: 0.15
        output_per_million: 0.60
        context_window: 128000
        description: "Very affordable for high-volume tasks"
      - label: "o3 (reasoning)"
        provider: "OpenAI"
        input_per_million: 2.00
        output_per_million: 8.00
        context_window: 200000
        description: "Advanced reasoning, chain-of-thought (hidden reasoning tokens billed as output)"
      - label: "o4-mini"
        provider: "OpenAI"
        input_per_million: 1.10
        output_per_million: 4.40
        context_window: 128000
        description: "Affordable reasoning model"

      # Anthropic
      - label: "Claude Opus 4.6"
        provider: "Anthropic"
        input_per_million: 5.00
        output_per_million: 25.00
        context_window: 200000
        description: "Most capable Claude (Feb 2026), excellent for complex tasks"
      - label: "Claude Sonnet 4.5"
        provider: "Anthropic"
        input_per_million: 3.00
        output_per_million: 15.00
        context_window: 200000
        description: "Best balance of intelligence and speed"
      - label: "Claude Haiku 4.5"
        provider: "Anthropic"
        input_per_million: 1.00
        output_per_million: 5.00
        context_window: 200000
        description: "Fast and affordable"

      # Google
      - label: "Gemini 3 Pro"
        provider: "Google"
        input_per_million: 2.00
        output_per_million: 12.00
        context_window: 1000000
        description: "Flagship model, strong multimodal, long context"
      - label: "Gemini 3 Flash"
        provider: "Google"
        input_per_million: 0.50
        output_per_million: 3.00
        context_window: 1000000
        description: "Fast and capable, free tier available"
      - label: "Gemini 2.5 Flash"
        provider: "Google"
        input_per_million: 0.30
        output_per_million: 2.50
        context_window: 1000000
        description: "Budget option, good for high-volume"

      # DeepSeek
      - label: "DeepSeek R1"
        provider: "DeepSeek"
        input_per_million: 0.55
        output_per_million: 2.19
        context_window: 128000
        description: "Reasoning model rivaling o1 at ~95% lower cost"
      - label: "DeepSeek V3"
        provider: "DeepSeek"
        input_per_million: 0.27
        output_per_million: 1.10
        context_window: 128000
        description: "GPT-4 class performance, extremely affordable"

    # Use case presets with typical token estimates
    use_cases:
      - label: "Document summarization"
        avg_input_tokens: 4000
        avg_output_tokens: 500
        description: "Summarize a 10-page document"
      - label: "Q&A / Chat"
        avg_input_tokens: 500
        avg_output_tokens: 300
        description: "Single question-answer exchange"
      - label: "Code generation"
        avg_input_tokens: 1000
        avg_output_tokens: 2000
        description: "Generate function or module"
      - label: "Data extraction"
        avg_input_tokens: 2000
        avg_output_tokens: 200
        description: "Extract structured data from text"
      - label: "Translation"
        avg_input_tokens: 1000
        avg_output_tokens: 1200
        description: "Translate document section"
      - label: "Literature review"
        avg_input_tokens: 8000
        avg_output_tokens: 1500
        description: "Analyze paper, extract key findings"
      - label: "Custom (manual entry)"
        avg_input_tokens: 0
        avg_output_tokens: 0
        description: "Enter your own token estimates"

    # Time period options for budgeting
    time_periods:
      - label: "Per request"
        multiplier: 1
      - label: "Daily"
        multiplier: 1
      - label: "Weekly"
        multiplier: 7
      - label: "Monthly"
        multiplier: 30
      - label: "Per semester (4 months)"
        multiplier: 120
      - label: "Per year"
        multiplier: 365
      - label: "Grant period (custom)"
        multiplier: 0

    # Tips for researchers
    tips:
      - "Token count ≈ words × 1.3 for English text"
      - "Code typically uses more tokens than prose"
      - "Input tokens are usually cheaper than output"
      - "Batch API (50% off) available from all major providers for non-urgent workloads"
      - "Prompt caching saves 50-90% on repeated content (e.g., system prompts, few-shot examples)"
      - "Reasoning models (o3, R1) use hidden 'thinking tokens' billed as output—actual costs may be 2-5× visible tokens"
      - "DeepSeek offers 50-75% off-peak discounts (16:30-00:30 GMT)"
      - "Azure AI Foundry: check your institution's enterprise agreement for negotiated rates"
      - "Prices change frequently - verify with IT before finalizing grants"

    # Factors that affect cost
    factors:
      - "Model choice (10-100× cost difference between providers)"
      - "Input vs output ratio"
      - "Request volume and timing (batch vs real-time)"
      - "Use of reasoning models (hidden token overhead)"
      - "Prompt caching eligibility"

# Global calculator settings
global:
  # Safety multiplier for all estimates
  safety_multiplier: 1.5
  safety_message: "Includes 1.5x buffer for processing intermediates"

  # Show calculation breakdown
  show_calculation: true

  # Precision for results
  storage_precision: 3    # decimal places for TB (= 1 GB precision, displayed as GB when < 1 TB)
  compute_precision: 0    # decimal places for SU

  # Archive ratio suggestion
  default_archive_ratio: 0.5
  archive_ratio_help: |
    Estimate what percentage of your data needs long-term retention.
    Typically 50% - raw data and final results are kept, intermediates
    are deleted.

  # Result display
  show_breakdown: true
  show_cost_estimate: true
  allow_manual_adjustment: true
